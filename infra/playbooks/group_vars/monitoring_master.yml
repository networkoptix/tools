prometheus_web_external_url: "http://{{ ansible_ssh_host }}:9090"

prometheus_global:
  scrape_interval: 10s
  scrape_timeout: 60s
  evaluation_interval: 15s

prometheus_alertmanager_config:
  - scheme: http
    static_configs:
      - targets:
        - "{{ ansible_ssh_host }}:9093"

prometheus_targets:
  node:
  - targets:
      "{{ groups['all'] | difference(groups['build_vm_win']) | map('extract', hostvars, ['node_exporter_address']) | map('regex_replace', '$', ':9100') | list }}"
    labels:
      env: monitoring
  alertmanager:
  - targets:
    - "{{ ansible_ssh_host }}:9093"
    labels:
      env: monitoring
  grafana:
  - targets:
    - "{{ ansible_ssh_host }}:3000"
    labels:
      env: monitoring

# Prometheus alerting rules
# https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/
prometheus_scrape_configs:
- job_name: "windows"
  metrics_path: "/metrics"
  static_configs:
  - targets:
      "{{ groups['build_vm_win'] | map('extract', hostvars, ['node_exporter_address']) | map('regex_replace', '$', ':9182') | list }}"
- job_name: "libvirt_nodes"
  metrics_path: "/metrics"
  static_configs:
  - targets:
      "{{ groups['libvirt_host'] | map('extract', hostvars, ['node_exporter_address']) | map('regex_replace', '$', ':9177') | list }}"
- job_name: "prometheus"
  metrics_path: "/metrics"
  static_configs:
  - targets:
    - "{{ ansible_ssh_host }}:9090"
- job_name: "node"
  file_sd_configs:
  - files:
    - "/etc/prometheus/file_sd/node.yml"
- job_name: "grafana"
  file_sd_configs:
  - files:
    - "/etc/prometheus/file_sd/grafana.yml"
- job_name: "alertmanager"
  file_sd_configs:
  - files:
    - "/etc/prometheus/file_sd/alertmanager.yml"

prometheus_environment_label: "Moscow"

prometheus_alert_rules:
  - alert: InstanceDown
    expr: "up == 0"
    for: 5m
    labels:
      severity: critical
      environment: "{{ prometheus_environment_label }}"
    annotations:
      description: "{% raw %}{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.{% endraw %}"
      summary: "{% raw %}Instance {{ $labels.instance }} down{% endraw %}"
  - alert: CriticalRAMUsage
    expr: '(1 - ((node_memory_MemFree + node_memory_Buffers + node_memory_Cached) / node_memory_MemTotal)) * 100 > 98'
    for: 5m
    labels:
      severity: critical
      environment: "{{ prometheus_environment_label }}"
    annotations:
      description: "{% raw %}{{ $labels.instance }} has Critical Memory Usage more than 5 minutes.{% endraw %}"
      summary: "{% raw %}Instance {{ $labels.instance }} has Critical Memory Usage{% endraw %}"
  - alert: CriticalDiskSpace
    expr: 'node_filesystem_free{job="node",filesystem!~"^/run(/|$)"} / node_filesystem_size{job="node"} < 0.1'
    for: 4m
    labels:
      severity: critical
      environment: "{{ prometheus_environment_label }}"
    annotations:
      description: "{% raw %}{{ $labels.instance }} of job {{ $labels.job }} has less than 10% space remaining.{% endraw %}"
      summary: "{% raw %}Instance {{ $labels.instance }} - Critical disk space usage{% endraw %}"
  - alert: CriticalDiskSpace
    expr: 'wmi_logical_disk_free_bytes{volume="C:"} / wmi_logical_disk_size_bytes{volume="C:"} < 0.1'
    for: 4m
    labels:
      severity: critical
      environment: "{{ prometheus_environment_label }}"
    annotations:
      description: "{% raw %}{{ $labels.instance }} of job {{ $labels.job }} has less than 10% space remaining.{% endraw %}"
      summary: "{% raw %}Instance {{ $labels.instance }} - Critical disk space usage{% endraw %}"
  - alert: RebootRequired
    expr: "node_reboot_required > 0"
    labels:
      severity: warning
      environment: "{{ prometheus_environment_label }}"
    annotations:
      description: "{% raw %}{{ $labels.instance }} requires a reboot.{% endraw %}"
      summary: "{% raw %}Instance {{ $labels.instance }} - reboot required{% endraw %}"

grafana_security:
  admin_user: admin
  admin_password: "{{ grafana_database_password }}"

grafana_auth:
  anonymous:
    org_name: "NetworkOptix"
    org_role: Viewer

prometheus_grafana_datasource: "Prometheus"

grafana_datasources:
  - name: "{{ prometheus_grafana_datasource }}"
    type: "prometheus"
    access: "proxy"
    url: "http://{{ ansible_ssh_host }}:9090"
    isDefault: true

#  Grafana role uses dashbordId to download a dashbord by request
# https://grafana.com/api/dashboards/{{ dashboard_id }}/revisions/{{ revision_id }}/download"
#
# You can find information about dashboards and its revisions
# on https://grafana.com/dashboards page,
grafana_dashboards:
  # Node ExporterFull
  - dashboard_id: '1860'  # dashboardId
    revision_id: '11'     # node exporter full dashboard last revision
    datasource: '{{ prometheus_grafana_datasource }}'
  # Node Exporter Server Metrics
  - dashboard_id: '405'  # dashboardId
    revision_id: '6'     # Node Exporter Server Metrics dashboard last revision
    datasource: '{{ prometheus_grafana_datasource }}'
  # Node Exporter (pre 0.16.0)
  - dashboard_id: '718'  # dashboardId
    revision_id: '1'     # node exporter (0.16.0) dashboard last revision
    datasource: '{{ prometheus_grafana_datasource }}'
  # Overview of metrics from Prometheus 2.0
  - dashboard_id: '3662'  # dashboardId
    revision_id: '2'      # Prometheus metrics dashboard last revision
    datasource: '{{ prometheus_grafana_datasource }}'
  # Windows Node dashboard
  - dashboard_id: '2129'  # dashboardId
    revision_id:  '3'     # Windows node dashboard last revision
    datasource: '{{ prometheus_grafana_datasource }}'

# Prometheus alertmanager configuration
# https://prometheus.io/docs/alerting/configuration/
alertmanager_external_url: "http://{{ ansible_ssh_host }}:9093" # Alertmanager UI

alertmanager_smtp:
   # The default SMTP From header field.
  from: 'alertmanager@networkoptix.com'
  # The default SMTP smarthost used for sending emails, including port number.
  # Port number usually is 25, or 587 for SMTP over TLS (sometimes referred to as STARTTLS).
  # Example: smtp.example.org:587
  smarthost: '{{ prometheus_alertmanager_smtp_smarthost }}'
  # The default hostname to identify to the SMTP server.
  auth_username: '{{ prometheus_alertmanager_smtp_auth_username }}'
  # SMTP Auth using LOGIN and PLAIN.
  auth_password: '{{ prometheus_alertmanager_smtp_auth_password }}'


alertmanager_receivers:
  - name: email  # The unique name of the receiver.
    # Configurations for several notification integrations.
    email_configs:
    - to: 'anikitin@networkoptix.com'
    - to: 'vfedorov@networkoptix.com'

# The root node of the routing tree.
alertmanager_route:
  group_by: ['alertname', 'cluster']
  # How long to initially wait to send a notification for a group
  # of alerts. Allows to wait for an inhibiting alert to arrive or collect
  # more initial alerts for the same group. (Usually ~0s to few minutes.)
  group_wait: 30s
  # How long to wait before sending a notification about new alerts that
  # are added to a group of alerts for which an initial notification has
  # already been sent. (Usually ~5m or more.)
  group_interval: 5m
  # How long to wait before sending a notification again if it has already
  # been sent successfully for an alert. (Usually ~3h or more).
  repeat_interval: 4h
  receiver: email

mac_node_exporter_hosts:
  "{{ groups['build_mac'] | map('extract', hostvars, ['node_exporter_address']) | list }}"